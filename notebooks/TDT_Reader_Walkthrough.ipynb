{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052212c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§© TDT Reader Notebook\n",
    "# This notebook explains and demonstrates the functionality of `tdt_reader.py`.\n",
    "# It provides:\n",
    "# - utilities to load and summarize TDT blocks,\n",
    "# - auto-selection of stores (LFP, stim, epoc),\n",
    "# - and helper functions for quick feature sanity checks.\n",
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Optional, Tuple, Iterable\n",
    "import numpy as np\n",
    "import tdt\n",
    "\n",
    "BASELINE = (-0.200, 0.000)\n",
    "RESPONSE = (0.0125, 0.100)\n",
    "STIM_WIN = (-0.001, 0.005)\n",
    "\n",
    "def _as_seconds_duration(d) -> float:\n",
    "    return d.total_seconds() if hasattr(d, \"total_seconds\") else float(d)\n",
    "\n",
    "def _first_match(keys: Iterable[str], preds: Iterable) -> Optional[str]:\n",
    "    for k in keys:\n",
    "        for p in preds:\n",
    "            if p(k):\n",
    "                return k\n",
    "    return None\n",
    "\n",
    "def _mean_across_channels(x: np.ndarray) -> np.ndarray:\n",
    "    x = np.asarray(x)\n",
    "    if x.ndim == 2:\n",
    "        return x.mean(axis=0)\n",
    "    return x\n",
    "\n",
    "def _slice_by_seconds(data: np.ndarray, fs: float, t0: float, t1: float) -> np.ndarray:\n",
    "    i0 = max(0, int(np.floor(t0 * fs)))\n",
    "    i1 = min(len(data), int(np.ceil(t1 * fs)))\n",
    "    if i1 <= i0:\n",
    "        return np.asarray([], dtype=float)\n",
    "    return data[i0:i1]\n",
    "\n",
    "def _rms(x: np.ndarray) -> float:\n",
    "    x = np.asarray(x, dtype=float).ravel()\n",
    "    return float(np.sqrt(np.mean(x**2))) if x.size else np.nan\n",
    "\n",
    "@dataclass\n",
    "class AutoStores:\n",
    "    epoc: str\n",
    "    lfp: str\n",
    "    stim: Optional[str]\n",
    "\n",
    "@dataclass\n",
    "class StreamInfo:\n",
    "    fs: float\n",
    "    shape: Tuple[int, ...]\n",
    "    scale: Optional[float]\n",
    "\n",
    "@dataclass\n",
    "class BlockSummary:\n",
    "    path: str\n",
    "    duration_sec: float\n",
    "    n_streams: int\n",
    "    n_epocs: int\n",
    "    auto: AutoStores\n",
    "    n_events: int\n",
    "    stim_absmax_median: float\n",
    "    streams: Dict[str, StreamInfo]\n",
    "    epocs: Dict[str, Dict[str, int]]\n",
    "\n",
    "def read_block(path: str):\n",
    "    td = tdt.read_block(path)\n",
    "    try:\n",
    "        _ = getattr(td.info, \"blockpath\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            setattr(td.info, \"blockpath\", str(path))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return td\n",
    "\n",
    "def auto_select_stores(tdt_obj) -> AutoStores:\n",
    "    epoc_key = _first_match(\n",
    "        tdt_obj.epocs.keys(),\n",
    "        preds=[\n",
    "            lambda k: k.lower().startswith(\"pc\"),\n",
    "            lambda k: k.lower().startswith(\"pt\"),\n",
    "            lambda k: k.lower().startswith(\"u\"),\n",
    "            lambda k: True,\n",
    "        ],\n",
    "    )\n",
    "    if epoc_key is None:\n",
    "        raise RuntimeError(\"No epoc stores found in block.\")\n",
    "\n",
    "    lfp_key = _first_match(\n",
    "        tdt_obj.streams.keys(),\n",
    "        preds=[lambda k: \"lfp\" in k.lower(), lambda k: \"wav\" in k.lower()],\n",
    "    )\n",
    "    if lfp_key is None:\n",
    "        raise RuntimeError(\"No LFP/Wav-like stream found.\")\n",
    "\n",
    "    lower_streams = {k.lower(): k for k in tdt_obj.streams.keys()}\n",
    "    stim_key = None\n",
    "    for cand in (\"izn1\", \"ssig\", \"sout\"):\n",
    "        if cand in lower_streams:\n",
    "            stim_key = lower_streams[cand]\n",
    "            break\n",
    "    if stim_key is None:\n",
    "        stim_key = _first_match(tdt_obj.streams.keys(), [lambda k: k.lower().startswith(\"izn\")])\n",
    "\n",
    "    return AutoStores(epoc=epoc_key, lfp=lfp_key, stim=stim_key)\n",
    "\n",
    "def get_stream(tdt_obj, name: str) -> Tuple[float, np.ndarray, Optional[float]]:\n",
    "    s = tdt_obj.streams[name]\n",
    "    fs = float(getattr(s, \"fs\"))\n",
    "    data = np.asarray(s.data)\n",
    "    scale = getattr(s, \"scale\", None)\n",
    "    return fs, data, scale\n",
    "\n",
    "def get_event_onsets(tdt_obj, epoc_name: str) -> np.ndarray:\n",
    "    return np.asarray(tdt_obj.epocs[epoc_name].onset, dtype=float)\n",
    "\n",
    "def quick_summary(tdt_obj) -> dict:\n",
    "    duration = _as_seconds_duration(getattr(tdt_obj.info, \"duration\", 0.0))\n",
    "    auto = auto_select_stores(tdt_obj)\n",
    "\n",
    "    streams = {}\n",
    "    for k, s in tdt_obj.streams.items():\n",
    "        streams[k] = dict(\n",
    "            fs=float(getattr(s, \"fs\", np.nan)),\n",
    "            shape=list(np.asarray(s.data).shape),\n",
    "            scale=getattr(s, \"scale\", None),\n",
    "        )\n",
    "\n",
    "    epocs = {k: {\"n\": int(len(v.onset))} for k, v in tdt_obj.epocs.items()}\n",
    "\n",
    "    onsets = get_event_onsets(tdt_obj, auto.epoc) if auto.epoc else np.array([])\n",
    "    n_events = int(onsets.size)\n",
    "\n",
    "    stim_absmax_median = 0.0\n",
    "    if auto.stim is not None:\n",
    "        fs_stim, stim, _ = get_stream(tdt_obj, auto.stim)\n",
    "        vals = []\n",
    "        for t0 in onsets:\n",
    "            seg = _slice_by_seconds(_mean_across_channels(stim), fs_stim,\n",
    "                                    t0 + STIM_WIN[0], t0 + STIM_WIN[1])\n",
    "            if seg.size:\n",
    "                vals.append(float(np.max(np.abs(seg))))\n",
    "        stim_absmax_median = float(np.median(vals)) if vals else 0.0\n",
    "\n",
    "    return dict(\n",
    "        path=str(getattr(tdt_obj.info, \"blockpath\", \"\")) or \"UNKNOWN\",\n",
    "        duration_sec=float(duration),\n",
    "        n_streams=len(streams),\n",
    "        n_epocs=len(epocs),\n",
    "        auto=dict(epoc=auto.epoc.replace(\"/\", \"_\"),\n",
    "                  lfp=auto.lfp,\n",
    "                  stim=auto.stim),\n",
    "        n_events=n_events,\n",
    "        stim_absmax_median=stim_absmax_median,\n",
    "        streams=streams,\n",
    "        epocs=epocs,\n",
    "    )\n",
    "\n",
    "def epoch_lfp(tdt_obj, onsets: np.ndarray, lfp_name: str, window: Tuple[float, float]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    fs, data, _ = get_stream(tdt_obj, lfp_name)\n",
    "    x = _mean_across_channels(data)\n",
    "    pre, post = window\n",
    "    n_samples = int(round((post - pre) * fs))\n",
    "    t = np.linspace(pre, post, n_samples, endpoint=False)\n",
    "    ep = []\n",
    "    for t0 in onsets:\n",
    "        i0 = int(round((t0 + pre) * fs))\n",
    "        i1 = i0 + n_samples\n",
    "        if i0 < 0 or i1 > len(x):\n",
    "            continue\n",
    "        e = x[i0:i1]\n",
    "        if len(e) == n_samples:\n",
    "            ep.append(e)\n",
    "    return t, np.asarray(ep, dtype=float)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
